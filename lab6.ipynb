{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0Xd1mVUVComIbeFsWLR3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rin-niee/MPSI/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ЧАСТЬ 1"
      ],
      "metadata": {
        "id": "emHBbkNiMyJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8YCW-7-DIsog"
      },
      "outputs": [],
      "source": [
        "pip install pandas umap-learn transformers datasets scikit-learn torch evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import umap\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "zIfBcslBJzFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "wMHk4YXDKPdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Загрузка данных(был взят какой-то готовый датасетик из уже встроенных в библиотеку, 20000 новостей/статей, с фильтрацией по категориям). Датасет на английском языке, т.к данные могут быть точнее(?)"
      ],
      "metadata": {
        "id": "4cIsiaIVKQmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = None #здесь нужно выбирать категории конкретные, я решила взять всю совокупность из 20 категорий\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "texts = newsgroups.data\n",
        "labels = newsgroups.target\n",
        "label_names = newsgroups.target_names"
      ],
      "metadata": {
        "id": "A2EU31_sKSpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Урезанный датасетик для ускорения работы программы"
      ],
      "metadata": {
        "id": "LLLLYqORKWzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "samples_per_class = 50\n",
        "class_counts = defaultdict(int)\n",
        "\n",
        "texts_subset = []\n",
        "labels_subset = []\n",
        "\n",
        "combined = list(zip(texts, labels))\n",
        "random.shuffle(combined)  # перемешаем, чтобы классы шли не группами\n",
        "\n",
        "for text, label in combined:\n",
        "    if class_counts[label] < samples_per_class:\n",
        "        texts_subset.append(text)\n",
        "        labels_subset.append(label)\n",
        "        class_counts[label] += 1\n",
        "    if all(count >= samples_per_class for count in class_counts.values()):\n",
        "        break\n",
        "\n",
        "texts = texts_subset\n",
        "labels = labels_subset"
      ],
      "metadata": {
        "id": "xj_Y91ueKbWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Предобработка текста(лемматизируем и убираем все ненужные стоп-слова)"
      ],
      "metadata": {
        "id": "wYp8CloRKkEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # убрать пунктуацию\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "GxKkokcxKi4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чистенький текст:"
      ],
      "metadata": {
        "id": "znDhKlBiKna6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_texts = [preprocess(text) for text in texts]"
      ],
      "metadata": {
        "id": "c1VeoHOUKot5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Анализируем данные(распределение статей по категориям) - тут я вновь взяла старые данные, просто с указанием единички, чтобы было проще"
      ],
      "metadata": {
        "id": "JEkk2nIrKrbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts1 = newsgroups.data\n",
        "labels1 = newsgroups.target\n",
        "label_names1 = newsgroups.target_names"
      ],
      "metadata": {
        "id": "sc03hJr6K-KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_texts1 = [preprocess(text) for text in texts1]"
      ],
      "metadata": {
        "id": "pyEvl8qwLChz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Количество документов: {len(clean_texts1)}\")\n",
        "print(f\"Количество категорий: {len(label_names1)}\")\n",
        "sns.countplot(x=labels1) #категории взяты цифрами, потому что без них совсем уродливо\n",
        "plt.title(\"Распределение по категориям\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8LdG3EJ-KsYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Векторизируем с использованием специальной библиотеки"
      ],
      "metadata": {
        "id": "Ib4N4lIKLSsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=20000) #увеличила количество на всякий случай, чтобы было поточнее\n",
        "X = vectorizer.fit_transform(clean_texts)\n",
        "X1 = vectorizer.fit_transform(clean_texts1)"
      ],
      "metadata": {
        "id": "kKYOhx_TLTLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Кластеризация"
      ],
      "metadata": {
        "id": "WXJgsSvzLU-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=len(label_names1), random_state=42)\n",
        "clusters = kmeans.fit_predict(X1)"
      ],
      "metadata": {
        "id": "12UDk5H_LWO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применяем UMAP для уменьшения размерности до 3D\n",
        "umap_model = umap.UMAP(n_components=3, random_state=42)\n",
        "X_umap_3d = umap_model.fit_transform(X1.toarray())\n",
        "\n",
        "# Визуализируем результаты кластеризации в 3D\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X_umap_3d[:, 0], X_umap_3d[:, 1], X_umap_3d[:, 2],\n",
        "                     c=clusters, cmap='tab10', alpha=0.6)\n",
        "\n",
        "ax.set_title(\"UMAP 3D визуализация кластеров\")\n",
        "ax.set_xlabel(\"UMAP компонент 1\")\n",
        "ax.set_ylabel(\"UMAP компонент 2\")\n",
        "ax.set_zlabel(\"UMAP компонент 3\")\n",
        "plt.colorbar(scatter, ax=ax, label='Cluster ID')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uaROw8rLLXhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Сравнение меток настоящих и после кластеризации"
      ],
      "metadata": {
        "id": "ZaK8xTPbMAgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ari = adjusted_rand_score(labels1, clusters)\n",
        "sil_score = silhouette_score(X1, clusters)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")"
      ],
      "metadata": {
        "id": "F-U202fJL7JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Визуализация распределения по реальным категориям\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(x=labels, palette='coolwarm')\n",
        "plt.title(\"Распределение по реальным категориям\")\n",
        "plt.xlabel(\"Label ID\")\n",
        "plt.ylabel(\"Количество документов\")\n",
        "\n",
        "# Визуализация распределения по кластерам\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(x=clusters, palette='viridis')\n",
        "plt.title(\"Распределение по кластерам (KMeans)\")\n",
        "plt.xlabel(\"Cluster ID\")\n",
        "plt.ylabel(\"Количество документов\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Метрики качества кластеризации\n",
        "ari = adjusted_rand_score(labels1, clusters)\n",
        "sil_score = silhouette_score(X1, clusters)\n",
        "\n",
        "print(f\"📊 Adjusted Rand Index (ARI): {ari:.4f} — насколько хорошо кластеры совпали с метками\")\n",
        "print(f\"📈 Silhouette Score: {sil_score:.4f} — насколько чёткие и отделимые получились кластеры\")"
      ],
      "metadata": {
        "id": "Z2vM-PvXL_54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну, такое разделение неудивительно из-за огромного количества шума в данных и множества однотипных подкатегорий, потому разделение скорее всего бдует выглядеть не слишком реалистично\n",
        "\n"
      ],
      "metadata": {
        "id": "xmqao5lMMQHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Разделение на train / val / test"
      ],
      "metadata": {
        "id": "nStySJElMRag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train+val / test\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, labels, test_size=0.15, stratify=labels, random_state=42)\n",
        "\n",
        "#train / val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1765, stratify=y_trainval, random_state=42)\n",
        "# 0.1765 * 0.85 ≈ 0.15, чтобы val тоже была 15%\n",
        "\n",
        "print(f\"Train size: {X_train.shape[0]}\")\n",
        "print(f\"Validation size: {X_val.shape[0]}\")\n",
        "print(f\"Test size: {X_test.shape[0]}\")\n"
      ],
      "metadata": {
        "id": "OQ75AEGnMU2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ЧАСТЬ 2"
      ],
      "metadata": {
        "id": "UV-if-5PMXMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "_3LNEQUuMWZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут мы снова разделяем на выборки для лучшего обучения модели. Была взять тенировочная модель huggingface"
      ],
      "metadata": {
        "id": "TnXwsIJwOS_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_all = list(range(len(clean_texts)))\n",
        "\n",
        "idx_trainval, idx_test = train_test_split(idx_all, test_size=0.15, stratify=labels, random_state=42)\n",
        "idx_train, idx_val = train_test_split(idx_trainval, test_size=0.1765, stratify=[labels[i] for i in idx_trainval], random_state=42)\n",
        "\n",
        "X_train_texts = [clean_texts[i] for i in idx_train]\n",
        "y_train = [labels[i] for i in idx_train]\n",
        "\n",
        "X_val_texts = [clean_texts[i] for i in idx_val]\n",
        "y_val = [labels[i] for i in idx_val]\n",
        "\n",
        "X_test_texts = [clean_texts[i] for i in idx_test]\n",
        "y_test = [labels[i] for i in idx_test]"
      ],
      "metadata": {
        "id": "V1Q-IqbeOdC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подгружаем модельку"
      ],
      "metadata": {
        "id": "Kj1mG5O0OetM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_names))"
      ],
      "metadata": {
        "id": "gjsvthnqPYfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем класс для токенизации модели и хранения меточек классов"
      ],
      "metadata": {
        "id": "te8SOIOuQOgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "9lrgjdFDPcmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут мы используем класс на выборках"
      ],
      "metadata": {
        "id": "vWy-QWlOQUGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(X_train_texts, y_train)\n",
        "val_dataset = TextDataset(X_val_texts, y_val)\n",
        "test_dataset = TextDataset(X_test_texts, y_test)"
      ],
      "metadata": {
        "id": "7hJIfpjEPe7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаем основные настройки обучения модели"
      ],
      "metadata": {
        "id": "wJ7KXulDQWCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # eval_strategy заменен на evaluation_strategy\n",
        "    save_strategy=\"epoch\",  # добавлено save_strategy, чтобы совпало с evaluation_strategy\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")"
      ],
      "metadata": {
        "id": "J_2wlSvwPgNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для удобства выносим метрику"
      ],
      "metadata": {
        "id": "AdKv6i4FQqVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "dwtFLNsHPh-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция подсчета метрик"
      ],
      "metadata": {
        "id": "sAkoTsf9QzK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)"
      ],
      "metadata": {
        "id": "wS5NeTDsPioK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс для организации процесса обучения модели"
      ],
      "metadata": {
        "id": "_12jZCLZQ3yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "model=model,\n",
        "args=training_args,\n",
        "train_dataset=train_dataset,\n",
        "eval_dataset=val_dataset,\n",
        "compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "GzrPSgJ1Pjfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тренируем модель, смотрим на результаты"
      ],
      "metadata": {
        "id": "XmjgBQzqQwiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "nWMESeeCPkTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "s1ef1jRGPqh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ВЫВОД:\n",
        "модель постепенно улучшается, так как за три эпохи результат улучшился(согласно точности) с 41% до 50%, что в общем-то неплохо с учетом урезанного датасета(так как с полным датасетиком время заходило за масштабы дней, а не часов).\n",
        "Training Loss снизился, значит модель медленно улучшается.\n",
        "На 20 классах, конечно, 50% довольно мало и походит больше на слепое угадывание, но, возможно, опять же вина в схожести категорий(что хорошо видно по результатам класстеризации) и обрезанному датасету.\n",
        "Вероятно, стало было лучше при увеличении эпох, переработке категорий"
      ],
      "metadata": {
        "id": "IkGTzb38Puhr"
      }
    }
  ]
}