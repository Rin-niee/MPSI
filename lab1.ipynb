{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eab1511-2a3f-459a-844c-83988b40085e",
   "metadata": {},
   "source": [
    "Импортируем все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71c1ad82-b36e-4d02-befc-1ed3f8241864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natasha in c:\\anaconda\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pymorphy2 in c:\\anaconda\\lib\\site-packages (from natasha) (0.9.1)\n",
      "Requirement already satisfied: razdel>=0.5.0 in c:\\anaconda\\lib\\site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: navec>=0.9.0 in c:\\anaconda\\lib\\site-packages (from natasha) (0.10.0)\n",
      "Requirement already satisfied: slovnet>=0.6.0 in c:\\anaconda\\lib\\site-packages (from natasha) (0.6.0)\n",
      "Requirement already satisfied: yargy>=0.16.0 in c:\\anaconda\\lib\\site-packages (from natasha) (0.16.0)\n",
      "Requirement already satisfied: ipymarkup>=0.8.0 in c:\\anaconda\\lib\\site-packages (from natasha) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: intervaltree>=3 in c:\\anaconda\\lib\\site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\anaconda\\lib\\site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\anaconda\\lib\\site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\anaconda\\lib\\site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\anaconda\\lib\\site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install natasha scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "af037750-552c-4022-85d0-bee7a8f7a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbabc5b-1a44-4342-a20f-74576b2a47e6",
   "metadata": {},
   "source": [
    "1. Лемматизация и стемминг текста\n",
    "\n",
    "Эта функция выполняет лемматизацию и стемминг для русского текста с помощью библиотеки natasha\n",
    "1) Текст разбивается на предложения и слова\n",
    "2)  Слова преобразуются в нормальные формы\n",
    "3)  Загружаются предобученные векторные представления слов\n",
    "4)  Анализируется морфология\n",
    "После создается объект и разбивается на токены, добавляется морфологический анализ\n",
    "Затем прорабатывается цикл поиска лемм и добавления их в список\n",
    "В стемменге усекаются слова до основы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "78fe38d0-45bb-47b6-8fab-6a43d66c6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_and_stem(text):\n",
    "    segmenter = Segmenter()\n",
    "    Vocab = MorphVocab()\n",
    "    Embeddings = NewsEmbedding()\n",
    "    Tagger = NewsMorphTagger(Embeddings)\n",
    "    Stemmer = SnowballStemmer(\"russian\")\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(Tagger)\n",
    "    \n",
    "    lems = []\n",
    "    for token in doc.tokens:\n",
    "        if token.lemma:  \n",
    "            lems.append(token.lemma)\n",
    "        else:\n",
    "            parsed = morph.parse(token.text)[0]\n",
    "            lems.append(parsed.normal_form)\n",
    "    \n",
    "    stems = [Stemmer.stem(t.text) for t in doc.tokens]\n",
    "    \n",
    "    return lems, stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d8f2a-2ff4-42b9-a3eb-4c7ad4717d66",
   "metadata": {},
   "source": [
    "2. Функция для токенизации всех символов из ASCII\n",
    "    1) Все символы, не являющиеся частью ASCII, будут проигнорированы.\n",
    "    2) Текст будет разбит на токены.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e50b7f9b-441e-4eea-966e-651ed5fe8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    if isinstance(text, str):\n",
    "        token = [char.lower() for char in text]\n",
    "    elif isinstance(text, list):\n",
    "        token = [char.lower() for word in text for char in word]\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b63466-140e-482f-95ed-cf63798ee646",
   "metadata": {},
   "source": [
    "3. Функция для векторизации всех символов из ASCII\n",
    "    1) Векторизация текста с помощью CountVectorizer из sklearn.\n",
    "    2) Все символы, не являющиеся частью ASCII, будут проигнорированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6fbec00b-63bc-406b-9caf-36b76b1d9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(token):\n",
    "    char_freq = defaultdict(int)\n",
    "    for char in token:\n",
    "        char_freq[char] += 1\n",
    "\n",
    "    char_to_index = {char: idx for idx, char in enumerate(char_freq.keys())}\n",
    "\n",
    "    vectorized_output = np.zeros(len(char_freq), dtype=int)\n",
    "    for char in token:\n",
    "        vectorized_output[char_to_index[char]] += 1\n",
    "\n",
    "    return ' '.join(map(str, vectorized_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ce471-5085-49f4-a8ff-445caafe714b",
   "metadata": {},
   "source": [
    "4. Токенизация и векторизация текста после лемматизации и стемминга\n",
    "    1) Лемматизация и стемминг текста.\n",
    "    2) Токенизация ASCII символов.\n",
    "    3) Векторизация текста с использованием CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55b966-49ff-4ec6-9966-18605dacf4f2",
   "metadata": {},
   "source": [
    "Русскоязычный текст(лемматизированный и стеммированный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "1c513473-8669-430d-bc32-3d8559623839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизированный текст: ['по', 'вечер', 'над', 'ресторан', 'горячий', 'воздух', 'дик', 'и', 'глухой', ',', 'и', 'править', 'окрик', 'пьяный', 'весенний', 'и', 'тлетворный', 'дух', '.'] \n",
      " \n",
      "Cтеммированный текст: ['по', 'вечер', 'над', 'ресторан', 'горяч', 'воздух', 'дик', 'и', 'глух', ',', 'и', 'прав', 'окрик', 'пьян', 'весен', 'и', 'тлетворн', 'дух', '.']\n",
      "\n",
      "Токены ASCII: ['п', 'о', ' ', 'в', 'е', 'ч', 'е', 'р', 'а', 'м', ' ', 'н', 'а', 'д', ' ', 'р', 'е', 'с', 'т', 'о', 'р', 'а', 'н', 'а', 'м', 'и', ' ', 'г', 'о', 'р', 'я', 'ч', 'и', 'й', ' ', 'в', 'о', 'з', 'д', 'у', 'х', ' ', 'д', 'и', 'к', ' ', 'и', ' ', 'г', 'л', 'у', 'х', ',', ' ', 'и', ' ', 'п', 'р', 'а', 'в', 'и', 'т', ' ', 'о', 'к', 'р', 'и', 'к', 'а', 'м', 'и', ' ', 'п', 'ь', 'я', 'н', 'ы', 'м', 'и', ' ', 'в', 'е', 'с', 'е', 'н', 'н', 'и', 'й', ' ', 'и', ' ', 'т', 'л', 'е', 'т', 'в', 'о', 'р', 'н', 'ы', 'й', ' ', 'д', 'у', 'х', '.']\n",
      "\n",
      "Векторизация ASCII: 1 6 16 4 6 2 7 6 4 6 4 2 4 10 1 2 3 1 3 3 3 1 2 1 1 2 1 2 1 1\n",
      "\n",
      "Токенизация после лемматизации: ['п', 'о', 'в', 'е', 'ч', 'е', 'р', 'н', 'а', 'д', 'р', 'е', 'с', 'т', 'о', 'р', 'а', 'н', 'г', 'о', 'р', 'я', 'ч', 'и', 'й', 'в', 'о', 'з', 'д', 'у', 'х', 'д', 'и', 'к', 'и', 'г', 'л', 'у', 'х', 'о', 'й', ',', 'и', 'п', 'р', 'а', 'в', 'и', 'т', 'ь', 'о', 'к', 'р', 'и', 'к', 'п', 'ь', 'я', 'н', 'ы', 'й', 'в', 'е', 'с', 'е', 'н', 'н', 'и', 'й', 'и', 'т', 'л', 'е', 'т', 'в', 'о', 'р', 'н', 'ы', 'й', 'д', 'у', 'х', '.']\n",
      "\n",
      "Векторизация после лемматизации: 3 7 5 6 2 7 6 3 4 2 4 2 2 8 5 1 3 3 3 2 1 2 2 1\n"
     ]
    }
   ],
   "source": [
    "text = \"По вечерам над ресторанами Горячий воздух дик и глух, И правит окриками пьяными Весенний и тлетворный дух.\"\n",
    "\n",
    "lemm_text, stem_text = lemm_and_stem(text)\n",
    "print(\"Лемматизированный текст:\", lemm_text, '\\n', '\\nCтеммированный текст:', stem_text)\n",
    " \n",
    "tokens = tokenize(text)\n",
    "print(\"\\nТокены ASCII:\", tokens)\n",
    "\n",
    "vectorized = vectorize(text)\n",
    "print(\"\\nВекторизация ASCII:\", vectorized)\n",
    "\n",
    "tokenize_lemm = tokenize(lemm_text)\n",
    "print(\"\\nТокенизация после лемматизации:\", tokenize_lemm)\n",
    "vectors = vectorize(tokenize_lemm)\n",
    "print(\"\\nВекторизация после лемматизации:\", vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af678e-0991-4b56-8a73-8f88540705ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
