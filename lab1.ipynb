{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eab1511-2a3f-459a-844c-83988b40085e",
   "metadata": {},
   "source": [
    "Импортируем все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c1ad82-b36e-4d02-befc-1ed3f8241864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pymorphy2 in c:\\anaconda\\lib\\site-packages (from natasha) (0.9.1)\n",
      "Collecting razdel>=0.5.0 (from natasha)\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting navec>=0.9.0 (from natasha)\n",
      "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting slovnet>=0.6.0 (from natasha)\n",
      "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting yargy>=0.16.0 (from natasha)\n",
      "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting ipymarkup>=0.8.0 (from natasha)\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: intervaltree>=3 in c:\\anaconda\\lib\\site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\anaconda\\lib\\site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\anaconda\\lib\\site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\anaconda\\lib\\site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\anaconda\\lib\\site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
      "Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
      "   ---------------------------------------- 0.0/34.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/34.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/34.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/34.4 MB 2.8 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 2.1/34.4 MB 4.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.6/34.4 MB 3.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 3.1/34.4 MB 4.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.5/34.4 MB 3.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 5.5/34.4 MB 4.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 7.3/34.4 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 8.7/34.4 MB 5.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 10.5/34.4 MB 5.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 12.1/34.4 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 13.6/34.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 15.5/34.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 17.0/34.4 MB 6.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 18.9/34.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 20.4/34.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 22.3/34.4 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 24.1/34.4 MB 6.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 25.7/34.4 MB 6.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 27.5/34.4 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 28.8/34.4 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 30.9/34.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 32.5/34.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  34.3/34.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 34.4/34.4 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
      "Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: razdel, navec, yargy, slovnet, ipymarkup, natasha\n",
      "Successfully installed ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install natasha scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af037750-552c-4022-85d0-bee7a8f7a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbabc5b-1a44-4342-a20f-74576b2a47e6",
   "metadata": {},
   "source": [
    "1. Лемматизация и стемминг текста\n",
    "\n",
    "Эта функция выполняет лемматизацию и стемминг для русского текста с помощью библиотеки natasha\n",
    "1) Текст разбивается на предложения и слова\n",
    "2)  Слова преобразуются в нормальные формы\n",
    "3)  Загружаются предобученные векторные представления слов\n",
    "4)  Анализируется морфология\n",
    "После создается объект и разбивается на токены, добавляется морфологический анализ\n",
    "Затем прорабатывается цикл поиска лемм и добавления их в список\n",
    "В стемменге усекаются слова до основы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "78fe38d0-45bb-47b6-8fab-6a43d66c6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_and_stem(text):\n",
    "    segmenter = Segmenter()\n",
    "    Vocab = MorphVocab()\n",
    "    Embeddings = NewsEmbedding()\n",
    "    Tagger = NewsMorphTagger(Embeddings)\n",
    "    Stemmer = SnowballStemmer(\"russian\")\n",
    "    \n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(Tagger)\n",
    "    \n",
    "    lems = []\n",
    "    for token in doc.tokens:\n",
    "        if token.lemma:  \n",
    "            lemma = token.lemma\n",
    "            Vocab.lemmatize(token)  \n",
    "            lems.append(token.lemma)\n",
    "        else:\n",
    "            lems.append(token.text.lower()) \n",
    "\n",
    "    stems = [Stemmer.stem(t.text) for t in doc.tokens]\n",
    "    \n",
    "    return lems, stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d8f2a-2ff4-42b9-a3eb-4c7ad4717d66",
   "metadata": {},
   "source": [
    "2. Функция для токенизации всех символов из ASCII\n",
    "    1) Все символы, не являющиеся частью ASCII, будут проигнорированы.\n",
    "    2) Текст будет разбит на токены.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e50b7f9b-441e-4eea-966e-651ed5fe8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    ASII = re.sub(r'[^ -~]+', '', text)\n",
    "    tokens = ASII.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b63466-140e-482f-95ed-cf63798ee646",
   "metadata": {},
   "source": [
    "3. Функция для векторизации всех символов из ASCII\n",
    "    1) Векторизация текста с помощью CountVectorizer из sklearn.\n",
    "    2) Все символы, не являющиеся частью ASCII, будут проигнорированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6fbec00b-63bc-406b-9caf-36b76b1d9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    vector = CountVectorizer()\n",
    "    v_text = vector.fit_transform([' '.join(tokens)])\n",
    "\n",
    "    return v_text.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ce471-5085-49f4-a8ff-445caafe714b",
   "metadata": {},
   "source": [
    "4. Токенизация и векторизация текста после лемматизации и стемминга\n",
    "    1) Лемматизация и стемминг текста.\n",
    "    2) Токенизация ASCII символов.\n",
    "    3) Векторизация текста с использованием CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9398d906-2eb5-4817-8db5-ca787e0899e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_lemm_stem(lemms):\n",
    "    embeddings = NewsEmbedding()\n",
    "    final_vector = []\n",
    "    \n",
    "    for lemm in lemms:\n",
    "        if lemm.isalpha():\n",
    "            try:\n",
    "                vector = embeddings[lemm]\n",
    "                if vector is None:  \n",
    "                    vector = np.zeros(300)\n",
    "            except:\n",
    "                vector = np.zeros(300)\n",
    "            final_vector.append(vector)\n",
    "        else:\n",
    "            final_vector.append(np.zeros(300))\n",
    "    \n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55b966-49ff-4ec6-9966-18605dacf4f2",
   "metadata": {},
   "source": [
    "Русскоязычный текст(лемматизированный и стеммированный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1c513473-8669-430d-bc32-3d8559623839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизированный текст: ['по', 'вечерам', 'над', 'ресторанами', 'горячий', 'воздух', 'дик', 'и', 'глух', ',', 'и', 'правит', 'окриками', 'пьяными', 'весенний', 'и', 'тлетворный', 'дух', '.'] \n",
      " \n",
      "Cтеммированный текст: ['по', 'вечер', 'над', 'ресторан', 'горяч', 'воздух', 'дик', 'и', 'глух', ',', 'и', 'прав', 'окрик', 'пьян', 'весен', 'и', 'тлетворн', 'дух', '.']\n",
      "Векторизация после лемматизации: [[-0.4462508  -0.25771007 -0.10361444 ... -0.12318133  0.17685772\n",
      "   0.01288834]\n",
      " [ 0.06137407  0.01232695  0.18338987 ...  0.42134026 -0.26348916\n",
      "  -0.38667753]\n",
      " [-0.33192572 -0.25423947 -0.31180307 ... -0.83404738 -0.0029812\n",
      "  -0.16885023]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1074577   0.10019927 -0.17940037 ... -0.64201134  0.19201182\n",
      "   0.08100268]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "text = \"По вечерам над ресторанами Горячий воздух дик и глух, И правит окриками пьяными Весенний и тлетворный дух.\"\n",
    "\n",
    "lemm_text, stem_text = lemm_and_stem(text)\n",
    "print(\"Лемматизированный текст:\", lemm_text, '\\n', '\\nCтеммированный текст:', stem_text)\n",
    "\n",
    "vectors = v_lemm_stem(lemm_text)\n",
    "print(\"Векторизация после лемматизации:\", vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3012018e-85d3-40de-95a2-9b42a3a528e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Токены ASCII: ['Sun', 'of', 'the', 'sleepless!', 'Melancholy', 'star!', 'Whose', 'tearful', 'beam', 'glows', 'tremulously', 'far,', 'That', 'showst', 'the', 'darkness', 'thou', 'canst', 'not', 'dispel,', 'How', 'like', 'art', 'thou', 'to', 'joy', 'rememberd', 'well!']\n",
      "\n",
      "Векторизация ASCII: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "text1 = 'Sun of the sleepless! Melancholy star! Whose tearful beam glows tremulously far, That show’st the darkness thou canst not dispel, How like art thou to joy remember’d well!'\n",
    "\n",
    "tokens = tokenize(text1)\n",
    "print(\"\\nТокены ASCII:\", tokens)\n",
    "\n",
    "vectorized = vectorize(text1)\n",
    "print(\"\\nВекторизация ASCII:\", vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af678e-0991-4b56-8a73-8f88540705ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
