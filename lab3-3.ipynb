{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "d7d58e71-09ba-4cf3-b595-dd6fca43bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\anaconda\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pymorphy3 in c:\\anaconda\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: dawg2-python>=0.8.0 in c:\\anaconda\\lib\\site-packages (from pymorphy3) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\anaconda\\lib\\site-packages (from pymorphy3) (2.4.417150.4580142)\n",
      "Requirement already satisfied: setuptools>=68.2.2 in c:\\anaconda\\lib\\site-packages (from pymorphy3) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1fdec943-b7a7-4673-be77-4018735ce6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "407e83dd-6eea-466d-846d-94c68ffc7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396e495-6ec9-423d-9a6a-097f89002d47",
   "metadata": {},
   "source": [
    "Создаем класс с методами обработки текста для дальнейшей работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f1f062af-f0eb-49ef-882b-80c1be20a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_transformation:\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        #токенизация текста\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def lemmas(self, tokens: List[str]) -> List[str]:\n",
    "        #лемматизация полученных токенов\n",
    "        morph3 = MorphAnalyzer()\n",
    "        return [morph3.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "    def stemmas(self, tokens: List[str]) -> List[str]:\n",
    "        #стемминг к токенам\n",
    "        stemmer = SnowballStemmer(\"russian\")\n",
    "        return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    def vectors(self, tokens: List[str]) -> List[int]:\n",
    "        #приведение токенов в ниукальные векторные представления\n",
    "        token_to_dict_vectors = {word: idx for idx, word in enumerate(set(tokens))}\n",
    "        return [token_to_dict_vectors[word] for word in tokens]\n",
    "\n",
    "    def vectors_in_dict(self, tokens: List[str]) -> Dict[str, int]:\n",
    "        #преобразование векторов в словарь с уникальными индексами\n",
    "        return {word: i for i, word in enumerate(set(tokens))}\n",
    "\n",
    "    def remove_stop_words(self, tokens: List[str]) -> List[str]:\n",
    "        #Удаление стоп-слов из списка токенов.\n",
    "        stop_words = set(stopwords.words('russian')).union({'.', ',', ':', '?', '!', '—'})\n",
    "        return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    def BoW(self, tokens: List[str]) -> Dict[str, int]:\n",
    "        count = {}\n",
    "        for word in tokens:\n",
    "            count[word] = count.get(word, 0) + 1\n",
    "        return count\n",
    "\n",
    "    def tf(self, tokens: List[str]) -> Dict[str, float]:\n",
    "        #Вычисление TF\n",
    "        word_count = self.bag_of_words(tokens)\n",
    "        total_words = len(tokens)\n",
    "        return {word: count / total_words for word, count in word_count.items()}\n",
    "\n",
    "    def idf(self, texts: List[List[str]]) -> Dict[str, float]:\n",
    "        #Вычисление IDF\n",
    "        total_texts = len(texts)\n",
    "        all_words = set(word for text in texts for word in set(text))\n",
    "        return {word: math.log(total_texts / sum(word in text for text in texts)) for word in all_words}\n",
    "\n",
    "    def tf_idf(self, texts: List[List[str]], indexText: int) -> Dict[str, float]:\n",
    "        #Вычисление TF-IDF\n",
    "        tf = self.tf(texts[indexText])\n",
    "        idf = self.idf(texts)\n",
    "        return {word: tf[word] * idf.get(word, 0) for word in tf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "59599481-0578-4f2d-89a4-38a06d944bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, size, embed, hidden, lr=0.001):\n",
    "        \n",
    "        self.embed = embed\n",
    "        self.hidden = hidden\n",
    "        self.size = size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.embeddings = np.random.randn(size, embed) * 0.01\n",
    "\n",
    "        self.q_SA = np.random.randn(embed, hidden) * 0.01\n",
    "        self.k_SA = np.random.randn(embed, hidden) * 0.01\n",
    "        self.v_SA = np.random.randn(embed, hidden) * 0.01\n",
    "\n",
    "        self.outt = np.random.randn(hidden, size) * 0.01\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp = np.exp(x - np.max(x))\n",
    "        return exp / np.sum(exp)\n",
    "\n",
    "    def forward(self, input_indices):\n",
    "        self.input_indices = input_indices\n",
    "        self.x = self.embeddings[input_indices]\n",
    "\n",
    "        self.Q = np.dot(self.x, self.q_SA)\n",
    "        self.K = np.dot(self.x, self.k_SA)\n",
    "        self.V = np.dot(self.x, self.v_SA)\n",
    "\n",
    "        self.scores = np.dot(self.Q, self.K.T) / np.sqrt(self.embed)\n",
    "        self.weights = self.softmax(self.scores)\n",
    "\n",
    "        self.attended = np.dot(self.weights, self.V)\n",
    "        self.context = np.mean(self.attended, axis=0)\n",
    "    \n",
    "        self.logits = np.dot(self.context, self.outt)\n",
    "        self.probs = self.softmax(self.logits)\n",
    "\n",
    "        return self.probs\n",
    "\n",
    "    def backward(self, target_index):\n",
    "        dlogits = self.probs.copy()\n",
    "        dlogits[target_index] -= 1\n",
    "        dW_out = np.outer(self.context, dlogits)\n",
    "\n",
    "        dcontext = np.dot(dlogits, self.outt.T)\n",
    "\n",
    "        dattended = np.ones_like(self.attended) * dcontext / self.attended.shape[0]\n",
    "    \n",
    "        dV = np.dot(self.weights.T, dattended)\n",
    "        d_attn_weights = np.dot(dattended, self.V.T)\n",
    "    \n",
    "        d_scores = d_attn_weights * self.weights * (1 - self.weights)\n",
    "    \n",
    "        dQ = np.dot(d_scores, self.K) / np.sqrt(self.embed)\n",
    "        dK = np.dot(d_scores.T, self.Q) / np.sqrt(self.embed)\n",
    "    \n",
    "        dW_q = np.dot(self.x.T, dQ)\n",
    "        dW_k = np.dot(self.x.T, dK)\n",
    "        dW_v = np.dot(self.x.T, dV)\n",
    "    \n",
    "        dx_q = np.dot(dQ, self.q_SA.T)\n",
    "        dx_k = np.dot(dK, self.k_SA.T)\n",
    "        dx_v = np.dot(dV, self.v_SA.T)\n",
    "        dx = dx_q + dx_k + dx_v\n",
    "    \n",
    "        for i, idx in enumerate(self.input_indices):\n",
    "            self.embeddings[idx] -= self.lr * dx[i]\n",
    "    \n",
    "        self.outt -= self.lr * dW_out\n",
    "        self.q_SA -= self.lr * dW_q\n",
    "        self.k_SA -= self.lr * dW_k\n",
    "        self.v_SA -= self.lr * dW_v\n",
    "\n",
    "    def train_step(self, input_indices, target_index):\n",
    "        self.forward(input_indices)\n",
    "        self.backward(target_index)\n",
    "\n",
    "    def predict(self, input_indices):\n",
    "        probs = self.forward(input_indices)\n",
    "        return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d374e1-913a-44d3-b47d-8277c8bcfcd8",
   "metadata": {},
   "source": [
    "Создаем объект полученного класса с функциями преобразования текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "2e53f7c1-2ff1-4759-879d-caa71a456a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tt = Text_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "9e733312-ec76-497a-a5fb-79ec8bfa5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Когда человек сознательно или интуитивно выбирает себе в жизни какую-то цель, жизненную задачу, он невольно дает себе оценку. По тому, ради чего человек живет, можно судить и о его самооценке - низкой или высокой. Если человек живет, чтобы приносить людям добро, облегчать их страдания, давать людям радость, то он оценивает себя на уровне этой своей человечности. Он ставит себе цель, достойную человека. Только такая цель позволяет человеку прожить свою жизнь с достоинством и получить настоящую радость. Да, радость! Подумайте: если человек ставит себе задачей увеличивать в жизни добро, приносить людям счастье, какие неудачи могут его постигнуть? Не тому помочь? Но много ли людей не нуждаются в помощи? Если жить только для себя, своими мелкими заботами о собственном благополучии, то от прожитого не останется и следа. Если же жить для других, то другие сберегут то, чему служил, чему отдавал силы. Можно по-разному определять цель своего существования, но цель должна быть. Надо иметь и принципы в жизни. Одно правило в жизни должно быть у каждого человека, в его цели жизни, в его принципах жизни, в его поведении: надо прожить жизнь с достоинством, чтобы не стыдно было вспоминать. Достоинство требует доброты, великодушия, умения не быть эгоистом, быть правдивым, хорошим другом, находить радость в помощи другим.'\n",
    "\n",
    "partt = [\n",
    "    \"Если человек живет, чтобы приносить людям\",\n",
    "    \"Он ставит себе цель, достойную\",\n",
    "    \"Только такая цель позволяет человеку прожить свою жизнь с\",\n",
    "    \"Если жить только для себя, своими мелкими заботами о\",\n",
    "    \"Если же жить для других, то другие сберегут то, чему\",\n",
    "    \"Надо иметь и\",\n",
    "    \"Одно правило в жизни должно быть у каждого\",\n",
    "    \"Достоинство требует\",\n",
    "]\n",
    "\n",
    "res = [\n",
    "    \"добро\",\n",
    "    \"человека\",\n",
    "    \"достоинством\",\n",
    "    \"благополучии\",\n",
    "    \"служил\",\n",
    "    \"принципы\",\n",
    "    \"человека\",\n",
    "    \"доброты\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "2df94306-42fd-4716-b2ea-ea071cfb179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Tt.remove_stop_words(Tt.lemmas(Tt.tokenize(text)))\n",
    "\n",
    "# Векторизация словаря\n",
    "vocab = list(set(tokens))\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for w, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "e24204fe-254a-483d-842d-06fb770a363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471ac40-73f3-47cc-beb3-fc31380fbd10",
   "metadata": {},
   "source": [
    "Предсказание последнего слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "c949cf1c-0e1d-40be-918b-c3b8ea79fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase, label in zip(partt, res):\n",
    "    tokens = Tt.remove_stop_words(Tt.lemmas(Tt.tokenize(phrase)))\n",
    "    if len(tokens) < 2: continue\n",
    "    input_indices = [word_to_index.get(w, 0) for w in tokens if w in word_to_index]\n",
    "    label_index = word_to_index.get(label, 0)\n",
    "    X.append(input_indices)\n",
    "    y.append(label_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2d849-031e-4d1b-889a-6c7d0ff20260",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "1c7ce252-d294-4099-a16d-4a21c2246588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.304065021993404\n",
      "Epoch 100, Loss: 4.3040649576104695\n",
      "Epoch 200, Loss: 4.304064893088274\n",
      "Epoch 300, Loss: 4.304064828418661\n",
      "Epoch 400, Loss: 4.304064763593434\n",
      "Epoch 500, Loss: 4.304064698604365\n",
      "Epoch 600, Loss: 4.30406463344318\n",
      "Epoch 700, Loss: 4.304064568101564\n",
      "Epoch 800, Loss: 4.304064502571164\n",
      "Epoch 900, Loss: 4.304064436843572\n",
      "Epoch 1000, Loss: 4.304064370910341\n",
      "Epoch 1100, Loss: 4.304064304762967\n",
      "Epoch 1200, Loss: 4.304064238392899\n",
      "Epoch 1300, Loss: 4.304064171791529\n",
      "Epoch 1400, Loss: 4.3040641049501955\n",
      "Epoch 1500, Loss: 4.30406403786018\n",
      "Epoch 1600, Loss: 4.304063970512699\n",
      "Epoch 1700, Loss: 4.304063902898912\n",
      "Epoch 1800, Loss: 4.304063835009912\n",
      "Epoch 1900, Loss: 4.304063766836727\n",
      "Epoch 2000, Loss: 4.304063698370312\n",
      "Epoch 2100, Loss: 4.304063629601559\n",
      "Epoch 2200, Loss: 4.304063560521278\n",
      "Epoch 2300, Loss: 4.304063491120211\n",
      "Epoch 2400, Loss: 4.304063421389018\n",
      "Epoch 2500, Loss: 4.304063351318281\n",
      "Epoch 2600, Loss: 4.304063280898498\n",
      "Epoch 2700, Loss: 4.304063210120086\n",
      "Epoch 2800, Loss: 4.304063138973368\n",
      "Epoch 2900, Loss: 4.304063067448584\n",
      "Epoch 3000, Loss: 4.30406299553588\n",
      "Epoch 3100, Loss: 4.304062923225304\n",
      "Epoch 3200, Loss: 4.304062850506811\n",
      "Epoch 3300, Loss: 4.3040627773702544\n",
      "Epoch 3400, Loss: 4.304062703805385\n",
      "Epoch 3500, Loss: 4.304062629801847\n",
      "Epoch 3600, Loss: 4.304062555349179\n",
      "Epoch 3700, Loss: 4.30406248043681\n",
      "Epoch 3800, Loss: 4.304062405054049\n",
      "Epoch 3900, Loss: 4.304062329190095\n",
      "Epoch 4000, Loss: 4.304062252834025\n",
      "Epoch 4100, Loss: 4.304062175974792\n",
      "Epoch 4200, Loss: 4.304062098601227\n",
      "Epoch 4300, Loss: 4.30406202070203\n",
      "Epoch 4400, Loss: 4.304061942265771\n",
      "Epoch 4500, Loss: 4.3040618632808805\n",
      "Epoch 4600, Loss: 4.304061783735658\n",
      "Epoch 4700, Loss: 4.304061703618256\n",
      "Epoch 4800, Loss: 4.3040616229166835\n",
      "Epoch 4900, Loss: 4.304061541618801\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(size=len(vocab), embed=16, hidden=32)\n",
    "\n",
    "for epoch in range(5000):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X)):\n",
    "        model.train_step(X[i], y[i])\n",
    "        probs = model.forward(X[i])\n",
    "        total_loss += -np.log(probs[y[i]] + 1e-9)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5d815-8674-4a7a-83a0-d10a49a95811",
   "metadata": {},
   "source": [
    "Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "72acfd95-b5c6-46d8-84c6-34b183dea189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "Если человек живет, чтобы приносить людям -> правдивый\n",
      "Он ставит себе цель, достойную -> выбирать\n",
      "Только такая цель позволяет человеку прожить свою жизнь с -> прожить\n",
      "Если жить только для себя, своими мелкими заботами о -> радость\n",
      "Если же жить для других, то другие сберегут то, чему -> задача\n",
      "Надо иметь и -> определять\n",
      "Одно правило в жизни должно быть у каждого -> радость\n",
      "Достоинство требует -> сила\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "for i in partt:\n",
    "    tokens = Tt.remove_stop_words(Tt.lemmas(Tt.tokenize(i)))\n",
    "    input_indices = [word_to_index.get(w, 0) for w in tokens if w in word_to_index]\n",
    "    pred_index = model.predict(input_indices)\n",
    "    print(f\"{i} -> {index_to_word[pred_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623e840-0610-40b4-992c-10b84dd6ad3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
